{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env.P[state][action] probability,nextstate,reward,isterminal\n",
    "\n",
    "16 states, 4 actions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_states=env.observation_space.n\n",
    "num_actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Valueiteration(env, value_vector, gamma, max_iters, epsilon):\n",
    "    num_iters = 0\n",
    "    value_curr = value_vector.copy()\n",
    "    value_next = value_vector.copy()\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        num_iters += 1\n",
    "        for state in env.P:\n",
    "            max_action = 0 \n",
    "            for action in env.P[state]:\n",
    "                temp = 0\n",
    "                for p, next_state, reward, isTerminal in env.P[state][action]:\n",
    "                    temp += p * (reward + gamma * value_curr[next_state])\n",
    "                max_action = max(max_action, temp)\n",
    "            value_next[state] = max_action\n",
    "\n",
    "        if np.max(np.abs(value_next - value_curr)) < epsilon:\n",
    "            value_curr = value_next.copy()\n",
    "            break\n",
    "        value_curr = value_next.copy()\n",
    "    print(num_iters)\n",
    "    return value_curr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01515046, 0.01535729, 0.02728922, 0.01550279, 0.02655761,\n",
       "       0.        , 0.05967895, 0.        , 0.05814652, 0.13358585,\n",
       "       0.19659954, 0.        , 0.        , 0.24638195, 0.54410951,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma=0.8\n",
    "max_iters=1000\n",
    "epsilon=1e-4\n",
    "value_vector=np.zeros(num_states)\n",
    "Valueiteration(env,value_vector,gamma,max_iters,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic policy Evaluation and Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_stochastic_policy(env,value_vector,policy,gamma,max_iters,epsilon):\n",
    "    num_states=env.observation_space.n\n",
    "    value_curr=value_vector\n",
    "    for i in range(max_iters):\n",
    "        value_next=value_curr.copy()\n",
    "        for state in env.P:\n",
    "            outsum=0\n",
    "            for action in env.P[state]:\n",
    "                insum=0\n",
    "                for p,next_state,reward,isTerminal in env.P[state][action]:\n",
    "                    insum+=(p*((gamma*value_curr[next_state])+reward))\n",
    "                outsum+=insum*policy[state][action]\n",
    "            value_next[state]=outsum\n",
    "        if np.linalg.norm(value_next-value_curr)<epsilon:\n",
    "            value_curr=value_next.copy()\n",
    "            break\n",
    "        value_curr=value_next.copy()\n",
    "    return value_curr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_stochastic_policy(env,value_vector,gamma):\n",
    "    num_states=env.observation_space.n\n",
    "    num_actions=env.action_space.n\n",
    "\n",
    "    qvaluesMatrix=np.zeros((num_states,num_actions))\n",
    "\n",
    "    improvedPolicy=np.zeros((num_states,num_actions))\n",
    "     \n",
    "    for state in range(num_states):\n",
    "\n",
    "        for action in range(num_actions):\n",
    "  \n",
    "            for p,next_state,reward,isTerminal in env.P[state][action]:\n",
    "                qvaluesMatrix[state,action]+=(p*(reward+gamma*value_vector[next_state]))\n",
    "             \n",
    "  \n",
    "        bestaction=np.where(qvaluesMatrix[state,:]==np.max(qvaluesMatrix[state,:]))\n",
    "     \n",
    "        improvedPolicy[state,bestaction]=1/np.size(bestaction)\n",
    "    return improvedPolicy,qvaluesMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "policy=np.ones((num_states,num_actions))/num_actions\n",
    "gamma=0.8\n",
    "max_iters=1000\n",
    "epsilon=1e-5\n",
    "value_vector=np.zeros(16)\n",
    "\n",
    "num_iters=0\n",
    "\n",
    "for iteration in range(1000):\n",
    "    num_iters+=1\n",
    "    if (iteration == 0):\n",
    "        currentPolicy=policy\n",
    "    value_vector=follow_stochastic_policy(env,value_vector,currentPolicy,gamma,max_iters,epsilon)\n",
    "    improvedPolicy,qvaluesMatrix=improve_stochastic_policy(env,value_vector,gamma)\n",
    "    if np.allclose(currentPolicy,improvedPolicy):\n",
    "        currentPolicy=improvedPolicy\n",
    "        print(\"converged!\")\n",
    "        break\n",
    "    \n",
    "    currentPolicy=improvedPolicy\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.01542077 0.01557963 0.0274332  0.01567188 0.02683969 0.\n",
      " 0.05977567 0.         0.05840101 0.13377427 0.19672968 0.\n",
      " 0.         0.24653087 0.54419182 0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.5 , 0.5 , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(num_iters)\n",
    "print(value_vector)\n",
    "currentPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_deterministic_policy(env,value_vector,policy,gamma,max_iters,epsilon):\n",
    "    num_states=env.observation_space.n\n",
    "    value_curr=value_vector\n",
    "    for i in range(max_iters):\n",
    "        value_next=value_curr.copy()\n",
    "        for state in env.P:\n",
    "            insum=0\n",
    "            for p,next_state,reward,isTerminal in env.P[state][policy[state]]:\n",
    "                insum+=(p*((gamma*value_curr[next_state])+reward))\n",
    "            value_next[state]=insum\n",
    "        if np.linalg.norm(value_next-value_curr)<epsilon:\n",
    "            value_curr=value_next.copy()\n",
    "            break\n",
    "        value_curr=value_next.copy()\n",
    "    return value_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_deterministic_policy(env,value_vector,gamma):\n",
    "\n",
    "    num_states=env.observation_space.n\n",
    "    num_actions=env.action_space.n\n",
    "\n",
    "    improved_policy=np.zeros(num_states)\n",
    "    for state in range(num_states):\n",
    "        max_action=0\n",
    "        actions=[]\n",
    "        for action in range(num_actions):\n",
    "            action_sum=0\n",
    "            for p,next_state,reward,isTerminal in env.P[state][action]:\n",
    "                action_sum+=(p*(reward+gamma*value_vector[next_state]))\n",
    "            actions.append(action_sum)\n",
    "        max_action=actions.index(max(actions))\n",
    "        improved_policy[state]=max_action\n",
    "    return improved_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged!\n"
     ]
    }
   ],
   "source": [
    "policy=np.zeros(num_states)\n",
    "gamma=0.8\n",
    "max_iters=1000\n",
    "epsilon=1e-5\n",
    "value_vector=np.zeros(num_states)\n",
    "\n",
    "num_iters=0\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    num_iters+=1\n",
    "    if (iteration == 0):\n",
    "        currentPolicy=policy\n",
    "    value_vector=follow_deterministic_policy(env,value_vector,currentPolicy,gamma,max_iters,epsilon)\n",
    "    improvedPolicy=improve_deterministic_policy(env,value_vector,gamma)\n",
    "    if np.allclose(currentPolicy,improvedPolicy):\n",
    "        currentPolicy=improvedPolicy\n",
    "        print(\"converged!\")\n",
    "        break\n",
    "    \n",
    "    currentPolicy=improvedPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[0.01542235 0.01558301 0.02743906 0.01567911 0.02684107 0.\n",
      " 0.05977898 0.         0.05840331 0.13377752 0.19673287 0.\n",
      " 0.         0.24653411 0.54419385 0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 3., 2., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(num_iters)\n",
    "print(value_vector)\n",
    "currentPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (d) - Custom Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self,desc=['RRRRR','RWRRR','RWFWG','SRRRR','CCCCC'],eta=0.2):\n",
    "        \"\"\" \n",
    "        0:Left\n",
    "        1:Down\n",
    "        2:Right\n",
    "        3:Up\n",
    "        \"\"\"\n",
    "        self.action_space=spaces.Discrete(4)\n",
    "        \"\"\" \n",
    "        15:Starting state\n",
    "        14:Goal State\n",
    "        \"\"\"\n",
    "        self.observation_space=spaces.Discrete(25)\n",
    "        self.agent_loc=15\n",
    "        self.goal_state=14\n",
    "        self.P=None\n",
    "        \"\"\" \n",
    "        S:Start State\n",
    "        W:Wall\n",
    "        R:Road\n",
    "        C:Cliff\n",
    "        F:Goal State 1(reward 1)\n",
    "        G:Goal State 2\n",
    "        \"\"\"\n",
    "        self.desc=desc\n",
    "        self.eta=eta\n",
    "\n",
    "    \"\"\" \n",
    "    Given s gives index in gridworld\n",
    "    \"\"\"\n",
    "    def state_to_index(self,s):\n",
    "        row=s //5\n",
    "        column=s- 5*row\n",
    "        return[row,column]\n",
    "    \n",
    "    \"\"\" \n",
    "    Gives reward for state\n",
    "    \"\"\"\n",
    "    def reward_state(self,s):\n",
    "        index=self.state_to_index(s)\n",
    "        if self.desc[index[0]][index[1]]=='W':\n",
    "            return 0.0\n",
    "        if self.desc[index[0]][index[1]]=='R' or self.desc[index[0]][index[1]]=='S':\n",
    "            return 0.0\n",
    "        if self.desc[index[0]][index[1]]=='C':\n",
    "            return -10.0\n",
    "        if self.desc[index[0]][index[1]]=='G':\n",
    "            return 10.0\n",
    "        if self.desc[index[0]][index[1]]=='F':\n",
    "            return 1.0\n",
    "    \n",
    "    \"\"\" \n",
    "    Returns if a state is terminal or not\n",
    "    \"\"\"\n",
    "    def terminal(self,s):\n",
    "        index=self.state_to_index(s)\n",
    "        if self.desc[index[0]][index[1]]=='W' or self.desc[index[0]][index[1]]=='C' or self.desc[index[0]][index[1]]=='F' or self.desc[index[0]][index[1]]=='G':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    \"\"\" \n",
    "    Returns next possible states from s\n",
    "    \"\"\"\n",
    "    def next_states(self,s):\n",
    "\n",
    "        next_states=[s-1,s+1,s-5,s+5]\n",
    "        if s%5==0:\n",
    "            next_states.pop(next_states.index(s-1))\n",
    "        if s<5:\n",
    "            next_states.pop(next_states.index(s-5))\n",
    "        if s>=20:\n",
    "            next_states.pop(next_states.index(s+5))\n",
    "        if (s+1)%5==0:\n",
    "            next_states.pop(next_states.index(s+1)) \n",
    "        return next_states\n",
    "            \n",
    "    def action_state(self,s,a):\n",
    "        next_state=None\n",
    "        if a==0:\n",
    "            if s%5!=0:\n",
    "                next_state=s-1\n",
    "            else:\n",
    "                next_state=s\n",
    "        elif a==1:\n",
    "            if s<20:\n",
    "                next_state=s+5\n",
    "            else:\n",
    "                next_state=s\n",
    "        elif a==2:\n",
    "            if (s+1)%5==0:\n",
    "                next_state=s\n",
    "            else:\n",
    "                next_state=s+1\n",
    "        elif a==3:\n",
    "            if s<5:\n",
    "                next_state=s\n",
    "            else:\n",
    "                next_state=s-5\n",
    "        return next_state\n",
    "\n",
    "    \"\"\" \n",
    "    returns env.P\n",
    "    \"\"\"\n",
    "    def set_P(self):\n",
    "        p={}\n",
    "        for s in range(self.observation_space.n):\n",
    "            index=self.state_to_index(s)\n",
    "            if self.desc[index[0]][index[1]]=='W' or  self.desc[index[0]][index[1]]=='C' or  self.desc[index[0]][index[1]]=='F' or  self.desc[index[0]][index[1]]=='G':\n",
    "                dictionary={\n",
    "                    0:[(1,s,0.0,True)],\n",
    "                    1:[(1,s,0.0,True)],\n",
    "                    2:[(1,s,0.0,True)],\n",
    "                    3:[(1,s,0.0,True)]\n",
    "                            }\n",
    "                p[s]=dictionary\n",
    "                continue\n",
    "            \n",
    "\n",
    "            dictionary={}\n",
    "            for action in range(self.action_space.n):\n",
    "                q=[]\n",
    "                next_state=self.action_state(s,action)\n",
    "                if next_state==s:\n",
    "                    next_s=self.next_states(s)\n",
    "                    q.append((1-self.eta,s,self.reward_state(s),self.terminal(s)))\n",
    "                    num=len(next_s)\n",
    "                    for j in range(num):\n",
    "                        q.append((self.eta/num,next_s[j],self.reward_state(next_s[j]),self.terminal(next_s[j])))\n",
    "                    dictionary[action]=q\n",
    "                else:\n",
    "                    q.append((1-self.eta,next_state,self.reward_state(next_state),self.terminal(next_state)))\n",
    "                    next_s=self.next_states(s)\n",
    "                    next_s.pop(next_s.index(next_state))\n",
    "                    num=len(next_s)\n",
    "                    for j in range(num):\n",
    "                        q.append((self.eta/num,next_s[j],self.reward_state(next_s[j]),self.terminal(next_s[j])))\n",
    "                    dictionary[action]=q\n",
    "            p[s]=dictionary\n",
    "        self.P=p\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1, 0, 0.0, False), (0.0, 1, 0.0, False), (0.0, 5, 0.0, False)],\n",
       "  1: [(1, 5, 0.0, False), (0.0, 1, 0.0, False)],\n",
       "  2: [(1, 1, 0.0, False), (0.0, 5, 0.0, False)],\n",
       "  3: [(1, 0, 0.0, False), (0.0, 1, 0.0, False), (0.0, 5, 0.0, False)]},\n",
       " 1: {0: [(1, 0, 0.0, False), (0.0, 2, 0.0, False), (0.0, 6, 0.0, True)],\n",
       "  1: [(1, 6, 0.0, True), (0.0, 0, 0.0, False), (0.0, 2, 0.0, False)],\n",
       "  2: [(1, 2, 0.0, False), (0.0, 0, 0.0, False), (0.0, 6, 0.0, True)],\n",
       "  3: [(1, 1, 0.0, False),\n",
       "   (0.0, 0, 0.0, False),\n",
       "   (0.0, 2, 0.0, False),\n",
       "   (0.0, 6, 0.0, True)]},\n",
       " 2: {0: [(1, 1, 0.0, False), (0.0, 3, 0.0, False), (0.0, 7, 0.0, False)],\n",
       "  1: [(1, 7, 0.0, False), (0.0, 1, 0.0, False), (0.0, 3, 0.0, False)],\n",
       "  2: [(1, 3, 0.0, False), (0.0, 1, 0.0, False), (0.0, 7, 0.0, False)],\n",
       "  3: [(1, 2, 0.0, False),\n",
       "   (0.0, 1, 0.0, False),\n",
       "   (0.0, 3, 0.0, False),\n",
       "   (0.0, 7, 0.0, False)]},\n",
       " 3: {0: [(1, 2, 0.0, False), (0.0, 4, 0.0, False), (0.0, 8, 0.0, False)],\n",
       "  1: [(1, 8, 0.0, False), (0.0, 2, 0.0, False), (0.0, 4, 0.0, False)],\n",
       "  2: [(1, 4, 0.0, False), (0.0, 2, 0.0, False), (0.0, 8, 0.0, False)],\n",
       "  3: [(1, 3, 0.0, False),\n",
       "   (0.0, 2, 0.0, False),\n",
       "   (0.0, 4, 0.0, False),\n",
       "   (0.0, 8, 0.0, False)]},\n",
       " 4: {0: [(1, 3, 0.0, False), (0.0, 9, 0.0, False)],\n",
       "  1: [(1, 9, 0.0, False), (0.0, 3, 0.0, False)],\n",
       "  2: [(1, 4, 0.0, False), (0.0, 3, 0.0, False), (0.0, 9, 0.0, False)],\n",
       "  3: [(1, 4, 0.0, False), (0.0, 3, 0.0, False), (0.0, 9, 0.0, False)]},\n",
       " 5: {0: [(1, 5, 0.0, False),\n",
       "   (0.0, 6, 0.0, True),\n",
       "   (0.0, 0, 0.0, False),\n",
       "   (0.0, 10, 0.0, False)],\n",
       "  1: [(1, 10, 0.0, False), (0.0, 6, 0.0, True), (0.0, 0, 0.0, False)],\n",
       "  2: [(1, 6, 0.0, True), (0.0, 0, 0.0, False), (0.0, 10, 0.0, False)],\n",
       "  3: [(1, 0, 0.0, False), (0.0, 6, 0.0, True), (0.0, 10, 0.0, False)]},\n",
       " 6: {0: [(1, 6, 0.0, True)],\n",
       "  1: [(1, 6, 0.0, True)],\n",
       "  2: [(1, 6, 0.0, True)],\n",
       "  3: [(1, 6, 0.0, True)]},\n",
       " 7: {0: [(1, 6, 0.0, True),\n",
       "   (0.0, 8, 0.0, False),\n",
       "   (0.0, 2, 0.0, False),\n",
       "   (0.0, 12, 1.0, True)],\n",
       "  1: [(1, 12, 1.0, True),\n",
       "   (0.0, 6, 0.0, True),\n",
       "   (0.0, 8, 0.0, False),\n",
       "   (0.0, 2, 0.0, False)],\n",
       "  2: [(1, 8, 0.0, False),\n",
       "   (0.0, 6, 0.0, True),\n",
       "   (0.0, 2, 0.0, False),\n",
       "   (0.0, 12, 1.0, True)],\n",
       "  3: [(1, 2, 0.0, False),\n",
       "   (0.0, 6, 0.0, True),\n",
       "   (0.0, 8, 0.0, False),\n",
       "   (0.0, 12, 1.0, True)]},\n",
       " 8: {0: [(1, 7, 0.0, False),\n",
       "   (0.0, 9, 0.0, False),\n",
       "   (0.0, 3, 0.0, False),\n",
       "   (0.0, 13, 0.0, True)],\n",
       "  1: [(1, 13, 0.0, True),\n",
       "   (0.0, 7, 0.0, False),\n",
       "   (0.0, 9, 0.0, False),\n",
       "   (0.0, 3, 0.0, False)],\n",
       "  2: [(1, 9, 0.0, False),\n",
       "   (0.0, 7, 0.0, False),\n",
       "   (0.0, 3, 0.0, False),\n",
       "   (0.0, 13, 0.0, True)],\n",
       "  3: [(1, 3, 0.0, False),\n",
       "   (0.0, 7, 0.0, False),\n",
       "   (0.0, 9, 0.0, False),\n",
       "   (0.0, 13, 0.0, True)]},\n",
       " 9: {0: [(1, 8, 0.0, False), (0.0, 4, 0.0, False), (0.0, 14, 10.0, True)],\n",
       "  1: [(1, 14, 10.0, True), (0.0, 8, 0.0, False), (0.0, 4, 0.0, False)],\n",
       "  2: [(1, 9, 0.0, False),\n",
       "   (0.0, 8, 0.0, False),\n",
       "   (0.0, 4, 0.0, False),\n",
       "   (0.0, 14, 10.0, True)],\n",
       "  3: [(1, 4, 0.0, False), (0.0, 8, 0.0, False), (0.0, 14, 10.0, True)]},\n",
       " 10: {0: [(1, 10, 0.0, False),\n",
       "   (0.0, 11, 0.0, True),\n",
       "   (0.0, 5, 0.0, False),\n",
       "   (0.0, 15, 0.0, False)],\n",
       "  1: [(1, 15, 0.0, False), (0.0, 11, 0.0, True), (0.0, 5, 0.0, False)],\n",
       "  2: [(1, 11, 0.0, True), (0.0, 5, 0.0, False), (0.0, 15, 0.0, False)],\n",
       "  3: [(1, 5, 0.0, False), (0.0, 11, 0.0, True), (0.0, 15, 0.0, False)]},\n",
       " 11: {0: [(1, 11, 0.0, True)],\n",
       "  1: [(1, 11, 0.0, True)],\n",
       "  2: [(1, 11, 0.0, True)],\n",
       "  3: [(1, 11, 0.0, True)]},\n",
       " 12: {0: [(1, 12, 0.0, True)],\n",
       "  1: [(1, 12, 0.0, True)],\n",
       "  2: [(1, 12, 0.0, True)],\n",
       "  3: [(1, 12, 0.0, True)]},\n",
       " 13: {0: [(1, 13, 0.0, True)],\n",
       "  1: [(1, 13, 0.0, True)],\n",
       "  2: [(1, 13, 0.0, True)],\n",
       "  3: [(1, 13, 0.0, True)]},\n",
       " 14: {0: [(1, 14, 0.0, True)],\n",
       "  1: [(1, 14, 0.0, True)],\n",
       "  2: [(1, 14, 0.0, True)],\n",
       "  3: [(1, 14, 0.0, True)]},\n",
       " 15: {0: [(1, 15, 0.0, False),\n",
       "   (0.0, 16, 0.0, False),\n",
       "   (0.0, 10, 0.0, False),\n",
       "   (0.0, 20, -10.0, True)],\n",
       "  1: [(1, 20, -10.0, True), (0.0, 16, 0.0, False), (0.0, 10, 0.0, False)],\n",
       "  2: [(1, 16, 0.0, False), (0.0, 10, 0.0, False), (0.0, 20, -10.0, True)],\n",
       "  3: [(1, 10, 0.0, False), (0.0, 16, 0.0, False), (0.0, 20, -10.0, True)]},\n",
       " 16: {0: [(1, 15, 0.0, False),\n",
       "   (0.0, 17, 0.0, False),\n",
       "   (0.0, 11, 0.0, True),\n",
       "   (0.0, 21, -10.0, True)],\n",
       "  1: [(1, 21, -10.0, True),\n",
       "   (0.0, 15, 0.0, False),\n",
       "   (0.0, 17, 0.0, False),\n",
       "   (0.0, 11, 0.0, True)],\n",
       "  2: [(1, 17, 0.0, False),\n",
       "   (0.0, 15, 0.0, False),\n",
       "   (0.0, 11, 0.0, True),\n",
       "   (0.0, 21, -10.0, True)],\n",
       "  3: [(1, 11, 0.0, True),\n",
       "   (0.0, 15, 0.0, False),\n",
       "   (0.0, 17, 0.0, False),\n",
       "   (0.0, 21, -10.0, True)]},\n",
       " 17: {0: [(1, 16, 0.0, False),\n",
       "   (0.0, 18, 0.0, False),\n",
       "   (0.0, 12, 1.0, True),\n",
       "   (0.0, 22, -10.0, True)],\n",
       "  1: [(1, 22, -10.0, True),\n",
       "   (0.0, 16, 0.0, False),\n",
       "   (0.0, 18, 0.0, False),\n",
       "   (0.0, 12, 1.0, True)],\n",
       "  2: [(1, 18, 0.0, False),\n",
       "   (0.0, 16, 0.0, False),\n",
       "   (0.0, 12, 1.0, True),\n",
       "   (0.0, 22, -10.0, True)],\n",
       "  3: [(1, 12, 1.0, True),\n",
       "   (0.0, 16, 0.0, False),\n",
       "   (0.0, 18, 0.0, False),\n",
       "   (0.0, 22, -10.0, True)]},\n",
       " 18: {0: [(1, 17, 0.0, False),\n",
       "   (0.0, 19, 0.0, False),\n",
       "   (0.0, 13, 0.0, True),\n",
       "   (0.0, 23, -10.0, True)],\n",
       "  1: [(1, 23, -10.0, True),\n",
       "   (0.0, 17, 0.0, False),\n",
       "   (0.0, 19, 0.0, False),\n",
       "   (0.0, 13, 0.0, True)],\n",
       "  2: [(1, 19, 0.0, False),\n",
       "   (0.0, 17, 0.0, False),\n",
       "   (0.0, 13, 0.0, True),\n",
       "   (0.0, 23, -10.0, True)],\n",
       "  3: [(1, 13, 0.0, True),\n",
       "   (0.0, 17, 0.0, False),\n",
       "   (0.0, 19, 0.0, False),\n",
       "   (0.0, 23, -10.0, True)]},\n",
       " 19: {0: [(1, 18, 0.0, False), (0.0, 14, 10.0, True), (0.0, 24, -10.0, True)],\n",
       "  1: [(1, 24, -10.0, True), (0.0, 18, 0.0, False), (0.0, 14, 10.0, True)],\n",
       "  2: [(1, 19, 0.0, False),\n",
       "   (0.0, 18, 0.0, False),\n",
       "   (0.0, 14, 10.0, True),\n",
       "   (0.0, 24, -10.0, True)],\n",
       "  3: [(1, 14, 10.0, True), (0.0, 18, 0.0, False), (0.0, 24, -10.0, True)]},\n",
       " 20: {0: [(1, 20, 0.0, True)],\n",
       "  1: [(1, 20, 0.0, True)],\n",
       "  2: [(1, 20, 0.0, True)],\n",
       "  3: [(1, 20, 0.0, True)]},\n",
       " 21: {0: [(1, 21, 0.0, True)],\n",
       "  1: [(1, 21, 0.0, True)],\n",
       "  2: [(1, 21, 0.0, True)],\n",
       "  3: [(1, 21, 0.0, True)]},\n",
       " 22: {0: [(1, 22, 0.0, True)],\n",
       "  1: [(1, 22, 0.0, True)],\n",
       "  2: [(1, 22, 0.0, True)],\n",
       "  3: [(1, 22, 0.0, True)]},\n",
       " 23: {0: [(1, 23, 0.0, True)],\n",
       "  1: [(1, 23, 0.0, True)],\n",
       "  2: [(1, 23, 0.0, True)],\n",
       "  3: [(1, 23, 0.0, True)]},\n",
       " 24: {0: [(1, 24, 0.0, True)],\n",
       "  1: [(1, 24, 0.0, True)],\n",
       "  2: [(1, 24, 0.0, True)],\n",
       "  3: [(1, 24, 0.0, True)]}}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "0:Left\n",
    "1:Down\n",
    "2:Right\n",
    "3:Up\n",
    "\"\"\"\n",
    "desc=['RRRRR','RWRRR','RWFWG','SRRRR','CCCCC']\n",
    "q=Gridworld(desc=desc,eta=0)\n",
    "\n",
    "q.set_P()\n",
    "q.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged!\n",
      "number of iterations: 6\n",
      "\n",
      "optimal policy for gamma=0.2, eta=0 is :\n",
      "[2. 2. 1. 1. 1. 1. 0. 1. 2. 1. 1. 0. 0. 0. 0. 2. 2. 3. 2. 3. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "converged!\n",
      "number of iterations: 6\n",
      "\n",
      "optimal policy for gamma=0.45, eta=0 is :\n",
      "[2. 2. 1. 1. 1. 1. 0. 2. 2. 1. 1. 0. 0. 0. 0. 2. 2. 2. 2. 3. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "converged!\n",
      "number of iterations: 6\n",
      "\n",
      "optimal policy for gamma=0.9, eta=0 is :\n",
      "[2. 2. 1. 1. 1. 1. 0. 2. 2. 1. 1. 0. 0. 0. 0. 2. 2. 2. 2. 3. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g=[0.2,0.45,0.9]\n",
    "for gam in g:\n",
    "    policy=np.zeros(q.observation_space.n)\n",
    "    gamma=gam\n",
    "    max_iters=1000\n",
    "    epsilon=1e-5\n",
    "    value_vector=np.zeros(q.observation_space.n)\n",
    "\n",
    "    num_iters=0\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        num_iters+=1\n",
    "        if (iteration == 0):\n",
    "            currentPolicy=policy\n",
    "        value_vector=follow_deterministic_policy(q,value_vector,currentPolicy,gamma,max_iters,epsilon)\n",
    "        improvedPolicy=improve_deterministic_policy(q,value_vector,gamma)\n",
    "        if np.allclose(currentPolicy,improvedPolicy):\n",
    "            currentPolicy=improvedPolicy\n",
    "            print(\"converged!\")\n",
    "            break\n",
    "        \n",
    "        currentPolicy=improvedPolicy\n",
    "    print(f'number of iterations: {num_iters}\\n')\n",
    "    print(f\"optimal policy for gamma={gam}, eta={q.eta} is :\\n{currentPolicy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged!\n",
      "number of iterations: 6\n",
      "\n",
      "optimal policy for gamma=0.9, eta=0 is :\n",
      "[2. 2. 1. 1. 1. 1. 0. 2. 2. 1. 1. 0. 0. 0. 0. 2. 2. 2. 2. 3. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "converged!\n",
      "number of iterations: 5\n",
      "\n",
      "optimal policy for gamma=0.9, eta=0.2 is :\n",
      "[2. 2. 2. 2. 1. 3. 0. 2. 2. 1. 3. 0. 0. 0. 0. 3. 2. 2. 2. 3. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n",
      "converged!\n",
      "number of iterations: 3\n",
      "\n",
      "optimal policy for gamma=0.9, eta=0.9 is :\n",
      "[1. 1. 0. 0. 0. 2. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "etas=[0,0.2,0.9]\n",
    "for et in etas:\n",
    "    q=Gridworld(desc=desc,eta=et)\n",
    "    q.set_P()\n",
    "    policy=np.zeros(q.observation_space.n)\n",
    "    gamma=0.9\n",
    "    max_iters=1000\n",
    "    epsilon=1e-5\n",
    "    value_vector=np.zeros(q.observation_space.n)\n",
    "\n",
    "    num_iters=0\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        num_iters+=1\n",
    "        if (iteration == 0):\n",
    "            currentPolicy=policy\n",
    "        value_vector=follow_deterministic_policy(q,value_vector,currentPolicy,gamma,max_iters,epsilon)\n",
    "        improvedPolicy=improve_deterministic_policy(q,value_vector,gamma)\n",
    "        if np.allclose(currentPolicy,improvedPolicy):\n",
    "            currentPolicy=improvedPolicy\n",
    "            print(\"converged!\")\n",
    "            break\n",
    "        \n",
    "        currentPolicy=improvedPolicy\n",
    "    print(f'number of iterations: {num_iters}\\n')\n",
    "    print(f\"optimal policy for gamma={gam}, eta={q.eta} is :\\n{currentPolicy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
